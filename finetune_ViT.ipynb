{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Fine-Tuning a Vision Transformer (ViT) on a Custom Dataset\n",
    "\n",
    "This notebook allows you to fine-tune a pre-trained Vision Transformer model from Hugging Face on your own image dataset.\n",
    "\n",
    "### How to Use This Notebook:\n",
    "1.  **Prepare Your Data**: Your training and validation images must be in `.zip` files.\n",
    "2.  **Follow the Steps**: Run each cell in order from top to bottom.\n",
    "3.  **Upload Your Data**: When prompted, upload your zipped dataset files.\n",
    "4.  **Train**: The model will train on your data.\n",
    "5.  **Download**: A final cell will let you download your trained model.\n",
    "\n",
    "### ‚ö†Ô∏è Important: Data Structure\n",
    "\n",
    "Your `.zip` files must contain folders where **each folder's name is the class label**. For example, if you have two classes, `cats` and `dogs`, your `train_data.zip` should have this structure:\n",
    "\n",
    "```\n",
    "train_data.zip\n",
    "‚îú‚îÄ‚îÄ cats/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cat_image_1.jpg\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cat_image_2.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ dogs/\n",
    "    ‚îú‚îÄ‚îÄ dog_image_1.jpeg\n",
    "    ‚îú‚îÄ‚îÄ dog_image_2.jpg\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\nThe same structure is required for your validation data `.zip` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup and Installations\n",
    "\n",
    "First, we'll install the necessary libraries from Hugging Face. We also check for GPU availability, as a GPU is highly recommended for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_bO9b4T4K-h",
    "outputId": "97e64177-3e1e-453d-815d-85dd4e723528"
   },
   "outputs": [],
   "source": [
    "%pip install -q transformers[torch] datasets accelerate\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available and print the device name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available. Using device: {torch.cuda.get_device_name(0)}\")\n",
    "    !nvidia-smi # Display GPU stats\n",
    "else:\n",
    "    print(\"GPU not available. Training will run on CPU, which will be very slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Here, you can set the key parameters for the training process. You can change the pre-trained model, the number of training epochs, and the batch size. Adjust the batch size based on your GPU's VRAM (if you get an 'Out of Memory' error, try reducing it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bw3z_30L4K-i"
   },
   "outputs": [],
   "source": [
    "# --- Main Configuration ---\n",
    "MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "OUTPUT_DIR = \"./output/\"\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "NUM_TRAIN_EPOCHS = 6\n",
    "# Adjust batch size based on your GPU's VRAM. \n",
    "# If you have a T4 (16GB), 32 should be safe. For an A100 (40GB), you can go higher.\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 32 \n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload and Unzip Datasets\n",
    "\n",
    "Run the cells below to upload your `training` and `validation` zip files. The code will automatically unzip them into the correct directories for the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_z24m-U4K-j",
    "outputId": "3418e69d-2104-43c2-d450-dd10ff2cd328"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "# --- Upload and Unzip Training Data ---\n",
    "print(\"Please upload your training data zip file.\")\n",
    "uploaded_train = files.upload()\n",
    "\n",
    "if not uploaded_train:\n",
    "  raise Exception(\"No training file was uploaded. Please restart the runtime and try again.\")\n",
    "\n",
    "train_zip_name = list(uploaded_train.keys())[0]\n",
    "TRAIN_DATA_DIR = os.path.splitext(train_zip_name)[0]\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(train_zip_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "print(f\"Successfully unzipped training data to '{TRAIN_DATA_DIR}'\")\n",
    "\n",
    "# --- Upload and Unzip Validation Data (Optional) ---\n",
    "VAL_DATA_DIR = None # Default to None\n",
    "print(\"\\n(Optional) Please upload your validation data zip file. If you don't have one, just press 'Cancel upload'.\")\n",
    "uploaded_val = files.upload()\n",
    "\n",
    "if uploaded_val:\n",
    "    val_zip_name = list(uploaded_val.keys())[0]\n",
    "    VAL_DATA_DIR = os.path.splitext(val_zip_name)[0]\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(val_zip_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    print(f\"Successfully unzipped validation data to '{VAL_DATA_DIR}'\")\n",
    "else:\n",
    "    print(\"No validation file uploaded. Training will proceed without a validation set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Data and Prepare for Training\n",
    "\n",
    "Now we load the images from the directories you just created. We'll use `torchvision`'s `ImageFolder` which automatically finds class labels from the folder names. We'll also define the model, tokenizer (processor), and the necessary functions for data collation and metrics calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_z24m-U4K-j"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from transformers import ViTImageProcessor, AutoModelForImageClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image # Pillow is needed for image handling by torchvision\n",
    "\n",
    "# --- 1. Load Data with torchvision ---\n",
    "if not os.path.exists(TRAIN_DATA_DIR):\n",
    "    raise FileNotFoundError(f\"Training data directory not found: {TRAIN_DATA_DIR}\")\n",
    "\n",
    "# Use torchvision's ImageFolder to load the datasets\n",
    "train_dataset = datasets.ImageFolder(TRAIN_DATA_DIR)\n",
    "\n",
    "# Get class names and mappings from the training dataset\n",
    "class_names = train_dataset.classes\n",
    "label2id = {name: i for i, name in enumerate(class_names)}\n",
    "id2label = {i: name for i, name in enumerate(class_names)}\n",
    "num_labels = len(class_names)\n",
    "\n",
    "print(f\"Found {num_labels} classes: {class_names}\")\n",
    "print(f\"Training data found: {len(train_dataset)} images.\")\n",
    "\n",
    "val_dataset = None\n",
    "if VAL_DATA_DIR and os.path.exists(VAL_DATA_DIR):\n",
    "    val_dataset = datasets.ImageFolder(VAL_DATA_DIR)\n",
    "    print(f\"Validation data found: {len(val_dataset)} images.\")\n",
    "else:\n",
    "    print(f\"Warning: Validation data directory not provided or not found. Skipping evaluation during training.\")\n",
    "\n",
    "\n",
    "# --- 2. Load Model and Processor ---\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True # Allows loading a pre-trained model with a different head\n",
    ")\n",
    "\n",
    "print(\"\\nModel and Processor loaded.\")\n",
    "\n",
    "# --- 3. Data Collator ---\n",
    "# This function processes batches of (PIL Image, label) tuples from ImageFolder\n",
    "# and prepares them for the model using the Hugging Face processor.\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    # The processor handles resizing, normalization, and tensor conversion.\n",
    "    batch_processor_output = processor(images, return_tensors=\"pt\")\n",
    "    batch_processor_output['labels'] = torch.tensor(labels)\n",
    "    return batch_processor_output\n",
    "\n",
    "print(\"Collate function defined.\")\n",
    "\n",
    "# --- 4. Define Metrics ---\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predicted_labels = predictions.argmax(axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predicted_labels)}\n",
    "\n",
    "print(\"Metrics function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Training\n",
    "\n",
    "Now we set up the `TrainingArguments` and the `Trainer`. \n",
    "\n",
    "- **`TrainingArguments`**: This object holds all the hyperparameters for the training run (like learning rate, batch size, etc.). We will use the parameters defined in the configuration step.\n",
    "- **`Trainer`**: This is the main Hugging Face class that orchestrates the entire training and evaluation loop.\n",
    "\n",
    "We also enable TensorBoard for real-time monitoring of the training loss and evaluation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_z24m-U4K-j"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Configure Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    # Use bf16 for faster training on compatible GPUs (like A100s in Colab Pro)\n",
    "    bf16=torch.cuda.is_bf16_supported(), \n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    # Set evaluation and saving strategies if a validation set is present\n",
    "    evaluation_strategy=\"epoch\" if val_dataset else \"no\",\n",
    "    save_strategy=\"epoch\" if val_dataset else \"no\",\n",
    "    load_best_model_at_end=True if val_dataset else False,\n",
    "    metric_for_best_model=\"accuracy\" if val_dataset else None,\n",
    "    greater_is_better=True if val_dataset else None,\n",
    "    # Report logs to TensorBoard\n",
    "    report_to=\"tensorboard\",\n",
    "    # Helps speed up data loading\n",
    "    dataloader_num_workers=2, \n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor, # The processor is passed as a tokenizer\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics if val_dataset else None,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")\n",
    "\n",
    "# Launch TensorBoard in the background (optional)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{OUTPUT_DIR}/logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start Training!\n",
    "\n",
    "This is the moment of truth. Running the cell below will start the fine-tuning process. You can monitor the progress here and in the TensorBoard panel above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_z24m-U4K-j"
   },
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate and Save the Final Model\n",
    "\n",
    "After training, we'll run a final evaluation on the validation set (if you provided one) to see how the best model performs. Then, we save the final model and its processor to the output directory. This saved model can be easily loaded later for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_z24m-U4K-j"
   },
   "outputs": [],
   "source": [
    "# --- Evaluate (Optional) ---\n",
    "if val_dataset:\n",
    "    print(\"\\nEvaluating the best model on the validation set...\")\n",
    "    metrics = trainer.evaluate()\n",
    "    print(\"Final evaluation metrics:\")\n",
    "    print(metrics)\n",
    "\n",
    "# --- Save the final model ---\n",
    "FINAL_MODEL_DIR = f\"{OUTPUT_DIR}/final_model\"\n",
    "print(f\"\\nSaving the final (or best) model and processor to {FINAL_MODEL_DIR}\")\n",
    "trainer.save_model(FINAL_MODEL_DIR)\n",
    "processor.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Your Trained Model\n",
    "\n",
    "The final step is to download the model you just trained. The cell below will zip the contents of the `final_model` directory and start a download in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_z24m-U4K-j"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Zip the final model directory\n",
    "output_filename = 'fine_tuned_vit_model'\n",
    "shutil.make_archive(output_filename, 'zip', FINAL_MODEL_DIR)\n",
    "\n",
    "print(f\"Model files zipped into '{output_filename}.zip'\")\n",
    "\n",
    "# Download the zipped file\n",
    "print(\"Starting download...\")\n",
    "files.download(f'{output_filename}.zip')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}